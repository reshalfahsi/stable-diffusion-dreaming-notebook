{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StableDiffusionDreaming.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshalfahsi/stable-diffusion-dreaming-notebook/blob/main/StableDiffusionDreaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stable Diffusion Dreaming**"
      ],
      "metadata": {
        "id": "T-UK1YOtPVdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "!mkdir -p \"/content/gdrive/My Drive/stablediffusion\"\n",
        "%cd \"/content/gdrive/My Drive/stablediffusion\""
      ],
      "metadata": {
        "id": "1gg9lOwf18gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install software-properties-common\n",
        "!apt update\n",
        "!add-apt-repository ppa:jonathonf/ffmpeg-4 -y\n",
        "!apt install ffmpeg\n",
        "!ffmpeg -version"
      ],
      "metadata": {
        "id": "tz2F0CaKS7oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers einops transformers huggingface-hub"
      ],
      "metadata": {
        "id": "6b8HC75KPQyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q4kWByBu11sy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "rootdir = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "UC88hdGzqAkA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "# To get token:\n",
        "# - Go to https://huggingface.co/settings/tokens\n",
        "# - Click at \"New token\"\n",
        "# - Set \"Name\" to \"stablediffusion\" and \"Role\" to \"write\"\n",
        "# - Click at \"Generate a token\"\n",
        "# - Copy the generated token"
      ],
      "metadata": {
        "id": "MpCb8S1qqK_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import inspect\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers.schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import autocast\n",
        "from torchvision.utils import make_grid"
      ],
      "metadata": {
        "id": "fWAyvQujPdlT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def diffuse(\n",
        "        pipe,\n",
        "        cond_embeddings, # text conditioning, should be (1, 77, 768)\n",
        "        cond_latents,    # image conditioning, should be (1, 4, 64, 64)\n",
        "        num_inference_steps,\n",
        "        guidance_scale,\n",
        "        eta,\n",
        "    ):\n",
        "    torch_device = cond_latents.get_device()\n",
        "\n",
        "    # classifier guidance: add the unconditional embedding\n",
        "    max_length = cond_embeddings.shape[1] # 77\n",
        "    uncond_input = pipe.tokenizer([\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "    text_embeddings = torch.cat([uncond_embeddings, cond_embeddings])\n",
        "\n",
        "    # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "    if isinstance(pipe.scheduler, LMSDiscreteScheduler):\n",
        "        cond_latents = cond_latents * pipe.scheduler.sigmas[0]\n",
        "\n",
        "    # init the scheduler\n",
        "    accepts_offset = \"offset\" in set(inspect.signature(pipe.scheduler.set_timesteps).parameters.keys())\n",
        "    extra_set_kwargs = {}\n",
        "    if accepts_offset:\n",
        "        extra_set_kwargs[\"offset\"] = 1\n",
        "    pipe.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "    # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "    # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "    # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "    # and should be between [0, 1]\n",
        "    accepts_eta = \"eta\" in set(inspect.signature(pipe.scheduler.step).parameters.keys())\n",
        "    extra_step_kwargs = {}\n",
        "    if accepts_eta:\n",
        "        extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "    # diffuse!\n",
        "    for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "\n",
        "        # expand the latents for classifier free guidance\n",
        "        latent_model_input = torch.cat([cond_latents] * 2)\n",
        "        if isinstance(pipe.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = pipe.scheduler.sigmas[i]\n",
        "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "        # predict the noise residual\n",
        "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # cfg\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        if isinstance(pipe.scheduler, LMSDiscreteScheduler):\n",
        "            cond_latents = pipe.scheduler.step(noise_pred, i, cond_latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "        else:\n",
        "            cond_latents = pipe.scheduler.step(noise_pred, t, cond_latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "    # scale and decode the image latents with vae\n",
        "    cond_latents = 1 / 0.18215 * cond_latents\n",
        "    image = pipe.vae.decode(cond_latents)\n",
        "\n",
        "    # generate output numpy image as uint8\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    image = (image[0] * 255).astype(np.uint8)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "YPWKKP4tPien"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "    \"\"\" helper function to spherically interpolate two arrays v1 v2 \"\"\"\n",
        "\n",
        "    if not isinstance(v0, np.ndarray):\n",
        "        inputs_are_torch = True\n",
        "        input_device = v0.device\n",
        "        v0 = v0.cpu().numpy()\n",
        "        v1 = v1.cpu().numpy()\n",
        "\n",
        "    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "    if np.abs(dot) > DOT_THRESHOLD:\n",
        "        v2 = (1 - t) * v0 + t * v1\n",
        "    else:\n",
        "        theta_0 = np.arccos(dot)\n",
        "        sin_theta_0 = np.sin(theta_0)\n",
        "        theta_t = theta_0 * t\n",
        "        sin_theta_t = np.sin(theta_t)\n",
        "        s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "        s1 = sin_theta_t / sin_theta_0\n",
        "        v2 = s0 * v0 + s1 * v1\n",
        "\n",
        "    if inputs_are_torch:\n",
        "        v2 = torch.from_numpy(v2).to(input_device)\n",
        "\n",
        "    return v2"
      ],
      "metadata": {
        "id": "PCx_maARP2vh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53\n",
        "# https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355\n",
        "\n",
        "def run(\n",
        "        # --------------------------------------\n",
        "        # args you probably want to change\n",
        "        prompt = \"blueberry spaghetti\", # prompt to dream about\n",
        "        gpu = 0, # id of the gpu to run on\n",
        "        name = 'blueberry', # name of this project, for the output directory\n",
        "        rootdir = '/home/ubuntu/dreams',\n",
        "        num_steps = 200, # number of steps between each pair of sampled points\n",
        "        max_frames = 1234, # number of frames to write and then exit the script\n",
        "        num_inference_steps = 50, # more (e.g. 100, 200 etc) can create slightly better images\n",
        "        guidance_scale = 7.5, # can depend on the prompt. usually somewhere between 3-10 is good\n",
        "        seed = 1337,\n",
        "        # --------------------------------------\n",
        "        # args you probably don't want to change\n",
        "        quality = 90, # for jpeg compression of the output images\n",
        "        eta = 0.0,\n",
        "        width = 512,\n",
        "        height = 512,\n",
        "        # --------------------------------------\n",
        "    ):\n",
        "    assert torch.cuda.is_available()\n",
        "    assert height % 8 == 0 and width % 8 == 0\n",
        "    torch.manual_seed(seed)\n",
        "    torch_device = f\"cuda:{gpu}\"\n",
        "\n",
        "    # init the output dir\n",
        "    outdir = os.path.join(rootdir, name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # init all of the models and move them to a given GPU\n",
        "    lms = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=lms, torch_dtype=torch.float16, use_auth_token=True)\n",
        "\n",
        "\n",
        "    pipe.unet.to(torch_device)\n",
        "    pipe.vae.to(torch_device)\n",
        "    pipe.text_encoder.to(torch_device)\n",
        "\n",
        "    # get the conditional text embeddings based on the prompt\n",
        "    text_input = pipe.tokenizer(prompt, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    cond_embeddings = pipe.text_encoder(text_input.input_ids.to(torch_device))[0] # shape [1, 77, 768]\n",
        "\n",
        "    # sample a source\n",
        "    init1 = torch.randn((1, pipe.unet.in_channels, height // 8, width // 8), device=torch_device)\n",
        "\n",
        "    # iterate the loop\n",
        "    frame_index = 0\n",
        "    while frame_index < max_frames:\n",
        "\n",
        "        # sample the destination\n",
        "        init2 = torch.randn((1, pipe.unet.in_channels, height // 8, width // 8), device=torch_device)\n",
        "\n",
        "        for i, t in enumerate(np.linspace(0, 1, num_steps)):\n",
        "            init = slerp(float(t), init1, init2)\n",
        "\n",
        "            print(\"dreaming... \", frame_index)\n",
        "            with autocast(\"cuda\"):\n",
        "                image = diffuse(pipe, cond_embeddings, init, num_inference_steps, guidance_scale, eta)\n",
        "            im = Image.fromarray(image)\n",
        "            outpath = os.path.join(outdir, 'frame%06d.jpg' % frame_index)\n",
        "            im.save(outpath, quality=quality)\n",
        "            frame_index += 1\n",
        "\n",
        "        init1 = init2"
      ],
      "metadata": {
        "id": "tK5JgqqDQGGN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Alien invasion of Mars colonization in the future.\"\n",
        "name = \"stablediffusion\"\n",
        "\n",
        "run(prompt=prompt, name=name, rootdir=rootdir)"
      ],
      "metadata": {
        "id": "Ryijm4HNQL9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -r 10 -f image2 -s 512x512 -i stablediffusion/frame%06d.jpg -vcodec libx264 -crf 10 -pix_fmt yuv420p result.mp4"
      ],
      "metadata": {
        "id": "VHiqp2-qTEnQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
